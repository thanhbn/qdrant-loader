# Core Concepts

Understanding these core concepts will help you make the most of QDrant Loader and troubleshoot issues effectively.

## ğŸ¯ Overview

QDrant Loader transforms your documents into searchable, AI-accessible knowledge. This guide explains the key concepts behind how it works.

## ğŸ§  Vector Databases and Embeddings

### What are Vector Databases?

A **vector database** stores information as high-dimensional numerical vectors that represent the semantic meaning of text, images, or other data.

```
Traditional Database:
"QDrant Loader is powerful" â†’ Stored as text

Vector Database:
"QDrant Loader is powerful" â†’ [0.1, -0.3, 0.8, 0.2, ...] (1536 dimensions)
```

### Why Vector Databases?

| Traditional Search | Vector Search |
|-------------------|---------------|
| Exact keyword matching | Semantic meaning matching |
| "QDrant Loader" finds only exact matches | "document ingestion tool" finds QDrant Loader |
| Limited context understanding | Understands relationships and context |
| Boolean results (match/no match) | Similarity scores (0.0 to 1.0) |

### What are Embeddings?

**Embeddings** are numerical representations of text that capture semantic meaning. Similar concepts have similar embeddings.

```python
# Example embeddings (simplified to 3 dimensions)
"dog" â†’ [0.8, 0.2, 0.1]
"puppy" â†’ [0.7, 0.3, 0.1]  # Similar to "dog"
"car" â†’ [0.1, 0.1, 0.9]    # Different from "dog"
```

### How QDrant Loader Uses Embeddings

1. **Text Chunking**: Documents are split into manageable chunks
2. **Embedding Generation**: Each chunk is converted to a vector using OpenAI's models
3. **Storage**: Vectors are stored in QDrant with metadata
4. **Search**: Query text is converted to a vector and matched against stored vectors

## ğŸ“„ Document Processing Pipeline

### Step 1: Data Source Connection

QDrant Loader connects to various data sources:

```
Data Sources â†’ QDrant Loader â†’ QDrant Database
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Git Repos   â”‚ â”€â”€â”
â”‚ Confluence  â”‚   â”‚
â”‚ JIRA        â”‚   â”œâ”€â†’ QDrant Loader â”€â”€â†’ Vector Database
â”‚ Local Files â”‚   â”‚
â”‚ Public Docs â”‚ â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 2: File Conversion

Documents are converted to plain text:

```
Input Formats â†’ Conversion â†’ Plain Text
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PDF         â”‚    â”‚              â”‚    â”‚             â”‚
â”‚ DOCX        â”‚ â”€â”€â†’â”‚ MarkItDown   â”‚â”€â”€â†’ â”‚ Plain Text  â”‚
â”‚ PPTX        â”‚    â”‚ Conversion   â”‚    â”‚ Content     â”‚
â”‚ Images      â”‚    â”‚              â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 3: Text Chunking

Large documents are split into smaller, manageable chunks:

```
Large Document â†’ Intelligent Chunking â†’ Smaller Chunks
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 50,000 words    â”‚    â”‚ Respect      â”‚    â”‚ Chunk 1     â”‚
â”‚ Technical Doc   â”‚ â”€â”€â†’â”‚ Boundaries   â”‚â”€â”€â†’ â”‚ Chunk 2     â”‚
â”‚                 â”‚    â”‚ Preserve     â”‚    â”‚ Chunk 3     â”‚
â”‚                 â”‚    â”‚ Context      â”‚    â”‚ ...         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Chunking Strategy**:

- **Respect boundaries**: Don't split sentences or code blocks
- **Maintain context**: Include overlapping content between chunks
- **Optimal size**: Balance between context and processing efficiency
- **Preserve structure**: Keep headings and formatting context

### Step 4: Embedding Generation

Each chunk is converted to a vector embedding:

```
Text Chunk â†’ OpenAI API â†’ Vector Embedding
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ "QDrant Loader  â”‚    â”‚ OpenAI       â”‚    â”‚ [0.1, -0.3, â”‚
â”‚ is a powerful   â”‚ â”€â”€â†’â”‚ text-embed-  â”‚â”€â”€â†’ â”‚  0.8, 0.2,  â”‚
â”‚ tool for..."    â”‚    â”‚ ding-3-small â”‚    â”‚  ...]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 5: Storage in QDrant

Vectors and metadata are stored in QDrant:

```
Vector + Metadata â†’ QDrant Collection
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Vector:         â”‚    â”‚ QDrant Collection            â”‚
â”‚ [0.1, -0.3,...] â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                 â”‚ â”€â”€â†’â”‚ â”‚ Vector  â”‚ Metadata        â”‚â”‚
â”‚ Metadata:       â”‚    â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚ - Source file   â”‚    â”‚ â”‚ [0.1..] â”‚ file: doc.md    â”‚â”‚
â”‚ - Chunk index   â”‚    â”‚ â”‚ [0.3..] â”‚ chunk: 1        â”‚â”‚
â”‚ - Content       â”‚    â”‚ â”‚ [0.8..] â”‚ source: git     â”‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”˜
```

## ğŸ” Search and Retrieval

### How Search Works

1. **Query Processing**: Your search query is converted to a vector
2. **Similarity Search**: QDrant finds vectors most similar to your query
3. **Ranking**: Results are ranked by similarity score
4. **Metadata Filtering**: Results can be filtered by source, date, etc.

```
Search Query â†’ Vector â†’ QDrant Search â†’ Ranked Results
     â†“
"API documentation" â†’ [0.2, 0.7, ...] â†’ QDrant â†’ [
  {score: 0.95, content: "API endpoints...", source: "api.md"},
  {score: 0.87, content: "REST API guide...", source: "guide.md"},
  {score: 0.82, content: "Authentication...", source: "auth.md"}
]
```

### Search Types

#### 1. Semantic Search (via MCP Server)

Finds content based on meaning, not just keywords. Search is performed through the MCP server tools:

```bash
# Search is performed via MCP server tools in AI applications
# Basic semantic search
{
  "name": "search",
  "arguments": {
    "query": "authentication methods",
    "limit": 10
  }
}
```

#### 2. Hierarchy Search (via MCP Server)

Combines semantic search with document structure awareness:

```bash
# Hierarchy-aware search via MCP server
{
  "name": "hierarchy_search", 
  "arguments": {
    "query": "API authentication",
    "organize_by_hierarchy": true
  }
}
```

#### 3. Attachment Search (via MCP Server)

Search within file attachments and their parent documents:

```bash
# Attachment search via MCP server
{
  "name": "attachment_search",
  "arguments": {
    "query": "deployment scripts",
    "attachment_filter": {
      "file_type": "sh"
    }
  }
}
```

## ğŸ¤– MCP Server and AI Integration

### What is MCP?

**Model Context Protocol (MCP)** is a standard for connecting AI tools to external data sources.

```
AI Tool (Cursor) â†â†’ MCP Server â†â†’ QDrant Database
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User asks:  â”‚    â”‚ MCP Server   â”‚    â”‚ QDrant      â”‚
â”‚ "How do I   â”‚ â”€â”€â†’â”‚ - Receives   â”‚â”€â”€â†’ â”‚ - Searches  â”‚
â”‚ configure   â”‚    â”‚   query      â”‚    â”‚   vectors   â”‚
â”‚ QDrant?"    â”‚    â”‚ - Calls      â”‚    â”‚ - Returns   â”‚
â”‚             â”‚ â†â”€â”€â”‚   search     â”‚â†â”€â”€ â”‚   results   â”‚
â”‚ Gets answer â”‚    â”‚ - Formats    â”‚    â”‚             â”‚
â”‚ with contextâ”‚    â”‚   response   â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### MCP Tools Available

QDrant Loader provides several MCP tools:

#### 1. Search Tool

Basic semantic search across all documents:

```json
{
  "name": "search",
  "description": "Search across all ingested documents",
  "parameters": {
    "query": "search terms",
    "limit": 10,
    "source_types": ["git", "confluence"]
  }
}
```

#### 2. Hierarchy Search Tool

Search with document structure awareness:

```json
{
  "name": "hierarchy_search", 
  "description": "Search with document hierarchy context",
  "parameters": {
    "query": "search terms",
    "organize_by_hierarchy": true,
    "hierarchy_filter": {
      "depth": 3,
      "has_children": true
    }
  }
}
```

#### 3. Attachment Search Tool

Search file attachments and their parent documents:

```json
{
  "name": "attachment_search",
  "description": "Search file attachments",
  "parameters": {
    "query": "search terms",
    "attachment_filter": {
      "file_type": "pdf",
      "include_parent_context": true
    }
  }
}
```

## ğŸ“Š Data Sources and Connectors

### Supported Data Sources

| Source | Description | Use Cases |
|--------|-------------|-----------|
| **Git Repositories** | Clone and index Git repos | Code documentation, README files |
| **Confluence** | Connect to Confluence spaces | Team wikis, knowledge bases |
| **JIRA** | Index JIRA issues and comments | Project documentation, requirements |
| **Local Files** | Process local directories | Personal documents, project files |
| **Public Documentation** | Scrape public websites | External API docs, tutorials |

### How Connectors Work

Each data source has a specialized connector:

```python
# Simplified connector interface
class DataSourceConnector:
    def connect(self, config):
        """Establish connection to data source"""
        
    def discover(self):
        """Find all available documents"""
        
    def fetch(self, document_id):
        """Retrieve document content"""
        
    def get_metadata(self, document_id):
        """Extract document metadata"""
```

### Incremental Updates

QDrant Loader tracks changes and only processes new or modified content:

```
Initial Sync: All documents processed
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document A      â”‚ â”€â”€â†’â”‚ Processed    â”‚
â”‚ Document B      â”‚ â”€â”€â†’â”‚ Processed    â”‚
â”‚ Document C      â”‚ â”€â”€â†’â”‚ Processed    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Incremental Sync: Only changes processed
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document A      â”‚ â”€â”€â†’â”‚ Skipped      â”‚ (unchanged)
â”‚ Document B      â”‚ â”€â”€â†’â”‚ Updated      â”‚ (modified)
â”‚ Document D      â”‚ â”€â”€â†’â”‚ Added        â”‚ (new)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âš™ï¸ Configuration and Customization

### Configuration Hierarchy

QDrant Loader uses a layered configuration system:

```
Environment Variables (highest priority)
     â†“
Configuration File (~/.qdrant-loader/config.yaml)
     â†“
Command Line Arguments
     â†“
Default Values (lowest priority)
```

### Project-Based Configuration

QDrant Loader uses a project-based configuration structure:

```yaml
# Multi-project configuration
projects:
  # Development project
  dev-project:
    project_id: "dev-project"
    display_name: "Development Documentation"
    description: "Development team documentation and code"
    
    sources:
      git:
        backend-repo:
          base_url: "https://github.com/company/backend"
          include_paths: ["docs/**", "README.md"]
          
      confluence:
        dev-space:
          base_url: "https://company.atlassian.net/wiki"
          space_key: "DEV"
          token: "${CONFLUENCE_TOKEN}"
          
  # Production project  
  prod-project:
    project_id: "prod-project"
    display_name: "Production Documentation"
    description: "Production systems and operations"
    
    sources:
      confluence:
        ops-space:
          base_url: "https://company.atlassian.net/wiki"
          space_key: "OPS"
          token: "${CONFLUENCE_TOKEN}"
```

### Key Configuration Areas

#### 1. Vector Database Settings

```yaml
qdrant:
  url: "http://localhost:6333"
  collection_name: "documents"
  vector_size: 1536
  distance_metric: "cosine"
```

#### 2. Embedding Configuration

```yaml
embedding:
  model: "text-embedding-3-small"
  api_key: "${OPENAI_API_KEY}"
  batch_size: 100
  endpoint: "https://api.openai.com/v1"
  vector_size: 1536
```

#### 3. Processing Settings

```yaml
chunking:
  chunk_size: 1500
  chunk_overlap: 200
  max_file_size: "10MB"
  supported_formats: ["md", "txt", "pdf", "docx"]
```

#### 4. Data Source Configuration

```yaml
sources:
  git:
    my-repo:
      base_url: "https://github.com/company/docs"
      branch: "main"
      include_paths: ["**/*.md", "**/*.rst"]
      exclude_paths: ["node_modules/", ".git/"]
  
  confluence:
    company-wiki:
      base_url: "https://company.atlassian.net/wiki"
      space_key: "DOCS"
      token: "${CONFLUENCE_TOKEN}"
      email: "${CONFLUENCE_EMAIL}"
```

## ğŸ”§ Performance and Optimization

### Understanding Performance Factors

#### 1. Document Size and Chunking

- **Larger chunks**: Better context, slower processing
- **Smaller chunks**: Faster processing, less context
- **Optimal size**: 500-1500 tokens per chunk

#### 2. Embedding Model Choice

| Model | Speed | Quality | Cost |
|-------|-------|---------|------|
| text-embedding-3-small | Fast | Good | Low |
| text-embedding-3-large | Slower | Better | Higher |
| text-embedding-ada-002 | Medium | Good | Medium |

#### 3. QDrant Configuration

```yaml
# Performance optimization
qdrant:
  # Use memory-mapped storage for large datasets
  storage_type: "mmap"
  
  # Optimize for search speed
  hnsw_config:
    m: 16
    ef_construct: 200
    
  # Batch operations
  batch_size: 100
```

### Monitoring Performance

```bash
# Check ingestion status
qdrant-loader project --workspace . status

# Monitor QDrant performance (direct API call)
curl http://localhost:6333/metrics

# Check collection statistics
qdrant-loader project --workspace . status --detailed
```

## ğŸ”— Integration Patterns

### Common Integration Scenarios

#### 1. Development Workflow

```
Code Changes â†’ Git Push â†’ Webhook â†’ QDrant Loader â†’ Updated Index
     â†“
Developer asks AI about code â†’ MCP Server â†’ Search â†’ Contextual answers
```

#### 2. Documentation Workflow

```
Wiki Updates â†’ Confluence â†’ Scheduled Sync â†’ QDrant Loader â†’ Updated Index
     â†“
Support team searches â†’ AI Tool â†’ MCP Server â†’ Accurate answers
```

#### 3. Knowledge Management

```
Multiple Sources â†’ QDrant Loader â†’ Unified Index â†’ AI Tools
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Git Repos   â”‚    â”‚              â”‚    â”‚ Cursor IDE  â”‚
â”‚ Confluence  â”‚ â”€â”€â†’â”‚ QDrant Loaderâ”‚â”€â”€â†’ â”‚ Windsurf    â”‚
â”‚ JIRA        â”‚    â”‚              â”‚    â”‚ Claude      â”‚
â”‚ Local Docs  â”‚    â”‚              â”‚    â”‚ Custom Apps â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ Best Practices

### 1. Data Organization

- **Use consistent naming**: Clear, descriptive file and folder names
- **Maintain structure**: Organize documents logically
- **Regular cleanup**: Remove outdated or duplicate content

### 2. Configuration Management

- **Environment-specific configs**: Different settings for dev/prod
- **Secure credentials**: Use environment variables for API keys
- **Version control**: Track configuration changes

### 3. Performance Optimization

- **Batch processing**: Process multiple documents together
- **Incremental updates**: Only sync changed content
- **Monitor resources**: Watch memory and API usage

### 4. Quality Assurance

- **Test search results**: Verify search quality regularly
- **Monitor accuracy**: Check AI responses for correctness
- **Update regularly**: Keep embeddings fresh with new content

## ğŸ” Troubleshooting Concepts

### Common Issues and Concepts

#### 1. Poor Search Results

**Cause**: Embedding model mismatch or poor chunking
**Solution**: Adjust chunk size or try different embedding model

#### 2. Slow Performance

**Cause**: Large chunks, inefficient QDrant config, or API rate limits
**Solution**: Optimize chunking, tune QDrant, implement rate limiting

#### 3. Memory Issues

**Cause**: Processing too many large documents simultaneously
**Solution**: Reduce batch size, process in smaller chunks

#### 4. Inconsistent Results

**Cause**: Outdated embeddings or mixed content types
**Solution**: Re-index content, separate different content types

## ğŸ“š Next Steps

Now that you understand the core concepts:

1. **[Basic Configuration](./basic-configuration.md)** - Set up your specific use case
2. **[User Guides](../users/)** - Explore detailed features
3. **[Data Source Guides](../users/detailed-guides/data-sources/)** - Configure specific connectors
4. **[MCP Server Guide](../users/detailed-guides/mcp-server/)** - Advanced AI integration

---

**Understanding these concepts will help you:**

- Configure QDrant Loader effectively for your use case
- Troubleshoot issues when they arise
- Optimize performance for your specific needs
- Make the most of AI tool integration
